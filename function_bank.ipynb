{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 事件区域生长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['0001_American_Beauty', '0002_As_Good_As_It_Gets', '0003_CASABLANCA', '0004_Charade', '0005_Chinatown', '0006_Clerks', '0007_DIE_NACHT_DES_JAEGERS', '0008_Fargo', '0009_Forrest_Gump', '0010_Frau_Ohne_Gewissen', '0011_Gandhi', '0012_Get_Shorty', '0013_Halloween', '0014_Ist_das_Leben_nicht_schoen', '0016_O_Brother_Where_Art_Thou', '0017_Pianist', '0019_Pulp_Fiction', '0020_Raising_Arizona', '0021_Rear_Window', '0022_Reservoir_Dogs', '0023_THE_BUTTERFLY_EFFECT', '0026_The_Big_Fish', '0027_The_Big_Lebowski', '0028_The_Crying_Game', '0029_The_Graduate', '0030_The_Hustler', '0031_The_Lost_Weekend', '0032_The_Princess_Bride', '0033_Amadeus', '0038_Psycho', '0041_The_Sixth_Sense', '0043_Thelma_and_Luise', '0046_Chasing_Amy', '0049_Hannah_and_her_sisters', '0050_Indiana_Jones_and_the_last_crusade', '0051_Men_in_black', '0053_Rendezvous_mit_Joe_Black', '1001_Flight', '1002_Harry_Potter_and_the_Half-Blood_Prince', '1003_How_to_Lose_Friends_and_Alienate_People', '1004_Juno', '1005_Signs', '1006_Slumdog_Millionaire', '1007_Spider-Man1', '1008_Spider-Man2', '1009_Spider-Man3', '1010_TITANIC', '1011_The_Help', '1012_Unbreakable', '10142', '10149', '1014_2012', '1015_27_Dresses', '1017_Bad_Santa', '1018_Body_Of_Lies', '1019_Confessions_Of_A_Shopaholic', '10202', '1020_Crazy_Stupid_Love', '1026_Legion', '1027_Les_Miserables', '1028_No_Reservations', '1031_Quantum_of_Solace', '10322', '1033_Sherlock_Holmes_A_Game_of_Shadows', '1034_Super_8', '1035_The_Adjustment_Bureau', '1037_The_Curious_Case_Of_Benjamin_Button', '1038_The_Great_Gatsby', '1039_The_Queen', '1040_The_Ugly_Truth', '1042_Up_In_The_Air', '1043_Vantage_Point', '1045_An_education', '1046_Australia', '1047_Defiance', '1048_Gran_Torino', '1050_Harry_Potter_and_the_deathly_hallows_Disk_One', '1051_Harry_Potter_and_the_goblet_of_fire', '10527', '1052_Harry_Potter_and_the_order_of_phoenix', '10536', '1054_Harry_Potter_and_the_prisoner_of_azkaban', '1055_Marley_and_me', '1057_Seven_pounds', '1058_The_Damned_united', '1059_The_devil_wears_prada', '1060_Yes_man', '1061_Harry_Potter_and_the_deathly_hallows_Disk_Two', '1062_Day_the_Earth_stood_still', '10784', '10813', '10836', '10861', '10894', '10965', '11003', '11010', '11099', '11129', '11139', '11140', '11143', '11147', '11148', '11154', '11318', '11321', '11345', '11396', '11430', '11438', '11530', '11620', '11727', '11796', '11962', '12010', '12079', '12090', '12125', '12131', '12132', '12144', '12147', '12148', '12186', '12211', '12220', '12222', '12263', '12273', '12294', '12324', '12358', '12504', '12563', '12585', '12618', '12653', '12658', '12743', '12852', '12869', '12900', '12906', '12911', '12923', '12958', '13018', '13027', '13031', '13045', '13140', '13146', '13159', '13165', '13187', '13191', '13201', '2723', '2730', '2731', '2735', '2738', '2745', '2750', '2758', '2768', '2778', '2787', '2800', '2801', '2814', '2818', '2854', '2869', '2870', '2873', '2911', '2913', '2928', '2934', '2944', '2948', '2970', '2986', '2992', '2996', '3001', '3001_21_JUMP_STREET', '3002_30_MINUTES_OR_LESS', '3003_40_YEAR_OLD_VIRGIN', '3004_500_DAYS_OF_SUMMER', '3005_ABRAHAM_LINCOLN_VAMPIRE_HUNTER', '3007_A_THOUSAND_WORDS', '3008_BAD_TEACHER', '3009_BATTLE_LOS_ANGELES', '3012_BRUNO', '3013_BURLESQUE', '3014', '3014_CAPTAIN_AMERICA', '3015_CHARLIE_ST_CLOUD', '3016_CHASING_MAVERICKS', '3017_CHRONICLE', '3018_CINDERELLA_MAN', '3020', '3020_DEAR_JOHN', '3021', '3021_DEATH_AT_A_FUNERAL', '3022_DINNER_FOR_SCHMUCKS', '3023', '3023_DISTRICT_9', '3024_EASY_A', '3025_FLIGHT', '3026_FRIENDS_WITH_BENEFITS', '3028_GHOST_RIDER_SPIRIT_OF_VENGEANCE', '3030_GROWN_UPS', '3031_HANSEL_GRETEL_WITCH_HUNTERS', '3032_HOW_DO_YOU_KNOW', '3033', '3033_HUGO', '3034_IDES_OF_MARCH', '3035_INSIDE_MAN', '3036_IN_TIME', '3037_IRON_MAN2', '3038_ITS_COMPLICATED', '3039_JACK_AND_JILL', '3040', '3040_JULIE_AND_JULIA', '3041_JUST_GO_WITH_IT', '3042_KARATE_KID', '3043_KATY_PERRY_PART_OF_ME', '3045_LAND_OF_THE_LOST', '3046_LARRY_CROWNE', '3047_LIFE_OF_PI', '3048_LITTLE_FOCKERS', '3049', '3049_MORNING_GLORY', '3050', '3050_MR_POPPERS_PENGUINS', '3051_NANNY_MCPHEE_RETURNS', '3052_NO_STRINGS_ATTACHED', '3053_PARENTAL_GUIDANCE', '3054_PERCY_JACKSON_LIGHTENING_THIEF', '3055_PROMETHEUS', '3056_PUBLIC_ENEMIES', '3058_RUBY_SPARKS', '3059', '3060', '3060_SANCTUM', '3061_SNOW_FLOWER', '3062_SORCERERS_APPRENTICE', '3063_SOUL_SURFER', '3066', '3066_THE_ADVENTURES_OF_TINTIN', '3067_THE_ART_OF_GETTING_BY', '3069_THE_BOUNTY_HUNTER', '3070', '3070_THE_CALL', '3071_THE_DESCENDANTS', '3072_THE_GIRL_WITH_THE_DRAGON_TATTOO', '3073_THE_GUILT_TRIP', '3074_THE_ROOMMATE', '3075_THE_SITTER', '3076_THE_SOCIAL_NETWORK', '3077_THE_VOW', '3078_THE_WATCH', '3079_THINK_LIKE_A_MAN', '3081_THOR', '3082_TITANIC1', '3083_TITANIC2', '3084_TOOTH_FAIRY', '3085_TRUE_GRIT', '3086_UGLY_TRUTH', '3087_WE_BOUGHT_A_ZOO', '3088_WHATS_YOUR_NUMBER', '3089_XMEN_FIRST_CLASS', '3090_YOUNG_ADULT', '3091_ZOMBIELAND', '3092_ZOOKEEPER', '3103', '3106', '3113', '3114', '3117', '3129', '3138', '3146', '3153', '3160', '3170', '3171', '3209', '3239', '3253', '3276', '3277', '3295', '3314', '3339', '3340', '3354', '3376', '3393', '3401', '3408', '3414', '3417', '3447', '3464', '3480', '3482', '3500', '3509', '3510', '3513', '3521', '3548', '3575', '3590', '3599', '3611', '3625', '3720', '3743', '3759', '3773', '3820', '3834', '3837', '3858', '3905', '3911', '3922', '3977', '4001', '4007', '4010', '4017', '4031', '4043', '4053', '4061', '4071', '4080', '4082', '4143', '4156', '4200', '4204', '4210', '4253', '4266', '4299', '4303', '4305', '4368', '4377', '4378', '4390', '4423', '4434', '4451', '4455', '4460', '4480', '4489', '4528', '4535', '4551', '4576', '4578', '4587', '4596', '4597', '4608', '4611', '4618', '4634', '4635', '4638', '4644', '4664', '4670', '4671', '4684', '4702', '4709', '4719', '4728', '4740', '4741', '4753', '4772', '4778', '4797', '4798', '4813', '4815', '4839', '4880', '4884', '4888', '4901', '4902', '4914', '4925', '4929', '4933', '4936', '4950', '4962', '4970', '4977', '4982', '4992', '5014', '5041', '5055', '5063', '5074', '5093', '5101', '5118', '5139', '5144', '5217', '5236', '5237', '5257', '5259', '5265', '5270', '5283', '5293', '5308', '5335', '5366', '5367', '5369', '5417', '5420', '5432', '5449', '5461', '5469', '5473', '5477', '5494', '5506', '5510', '5511', '5522', '5563', '5565', '5568', '5574', '5575', '5577', '5583', '5594', '5605', '5607', '5634', '5641', '5649', '5677', '5678', '5682', '5685', '5700', '5735', '5737', '5743', '5749', '5752', '5758', '5762', '5792', '5807', '5814', '5818', '5819', '5828', '5852', '5865', '5872', '5873', '5898', '5900', '5913', '5923', '5950', '5958', '6012', '6013', '6022', '6048', '6055', '6057', '6076', '6086', '6090', '6137', '6153', '6154', '6156', '6177', '6186', '6194', '6224', '6232', '6319', '6334', '6394', '6402', '6491', '6521', '6607', '6613', '6617', '6629', '6636', '6655', '6656', '6672', '6685', '6701', '6706', '6741', '6769', '6770', '6775', '6810', '6811', '6816', '6819', '6832', '6833', '6837', '6859', '6869', '6870', '6878', '6890', '6952', '6959', '6992', '6994', '7001', '7005', '7007', '7026', '7036', '7050', '7055', '7131', '7195', '7196', '7243', '7682', '7882', '8152', '8276', '8295', '8346', '8496', '8578', '8587', '8589', '8593', '8598', '8601', '8608', '8616', '8618', '8637', '8734', '8766', '8767', '8811', '9110', '9277', '9380', '9384', '9386', '9387', '9419', '9421', '9451', '9456', '9460', '9461', '9462', '9481', '9482', '9488', '9502', '9504', '9509', '9510', '9515', '9519', '9526', '9528', '9529', '9535', '9552', '9555', '9575', '9576', '9583', '9595', '9606', '9615', '9617', '9618', '9619', '9620', '9638', '9642', '9644', '9647', '9654', '9659', '9676', '9689', '9719', '9724', '9732', '9733', '9735', '9737', '9738', '9741', '9747', '9750', '9751', '9754', '9756', '9761', '9773', '9774', '9785', '9799', '9846', '9896', '9906', '9920', '9952']>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import time\n",
    "\n",
    "# 计算两帧特征之间的相似度（余弦相似度）\n",
    "def calculate_similarity(feature1, feature2):\n",
    "    dot_product = np.dot(feature1, feature2)\n",
    "    norm_a = np.linalg.norm(feature1)  #L2范数，表示向量的长度\n",
    "    norm_b = np.linalg.norm(feature2)\n",
    "    similarity = dot_product / (norm_a * norm_b) #向量积除以向量长度的乘积等于相似度\n",
    "    return similarity\n",
    "\n",
    "# 根据相似度进行区域生长\n",
    "def region_growing_event_clustering(features, similarity_threshold):\n",
    "    num_frames = len(features)\n",
    "    # 初始化事件标签为-1,一开始所有的帧都标记为没有归类所属事件，大小和帧数一样\n",
    "    events = np.zeros(num_frames, dtype=int) - 1  \n",
    "    current_event = 0  #事件的记号，依次递增表示第几个事件\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        #如果当前帧没有标记为任何事件，防止当前帧在聚集事件之后将相邻帧也进行了标记，后续还要对相邻帧进行计算的重复开销\n",
    "        if events[i] == -1:  #第一帧处理是没有事件标记的\n",
    "            events[i] = current_event\n",
    "            queue = [i]\n",
    "            while queue:\n",
    "                current_frame = queue.pop(0)\n",
    "                #neighbor也只是帧索引序号，当前帧的左右相邻两帧，左右两帧进行遍历计算\n",
    "                for neighbor in [current_frame - 1, current_frame + 1]:\n",
    "                    #相邻帧在合理的范围内，且没有被标记为任何事件\n",
    "                    if 0 <= neighbor < num_frames and events[neighbor] == -1:\n",
    "                        similarity = calculate_similarity(features[current_frame], features[neighbor])\n",
    "                        if similarity > similarity_threshold:\n",
    "                            events[neighbor] = current_event\n",
    "                            queue.append(neighbor)\n",
    "            current_event += 1\n",
    "    \n",
    "    return events\n",
    "\n",
    "# 主函数\n",
    "def main(features, similarity_threshold):\n",
    "    events = region_growing_event_clustering(features, similarity_threshold)\n",
    "        # print(\"事件划分结果：\", events)\n",
    "    return events\n",
    "\n",
    "\n",
    "# 示例：假设已经有视频帧特征，每个特征为长度为512的向量\n",
    "# features = np.random.rand(3618, 512)  # 用随机数模拟特征\n",
    "vfeat_path = '/mnt/hdd1/zhulu/mad/CLIP_B32_frames_features_5fps.h5'  #mad数据集1s采样5帧\n",
    "with h5py.File(vfeat_path, 'r') as f:\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    features=f['0001_American_Beauty'][:]\n",
    "# similarity_threshold = 0.75  # 根据需要调整阈值\n",
    "# events=main(features, similarity_threshold)\n",
    "# events\n",
    "\n",
    "# Forrest_Gump #阿甘正传"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def events_modify(events):\n",
    "    \"\"\"_summary_:将事件中出现次数低于5帧的事件进行相邻事件的合并，以避免因抖动、遮挡导致的事件划分不准确\n",
    "    先统计出所有的长度低于5帧的事件(不足1s)，然后遍历这些事件，从当前事件向左找第一个长度大于5帧的事件：三种情况\n",
    "    1：左边事件长度大于5帧，直接将当前事件的所有帧标记为左边事件\n",
    "    2：左边事件长度小于5帧，因为是for循环遍历的方式，本身事件就是从小到大的，因此遍历到的第一个事件就是最第一个长度小于5帧的事件，不存在此情况\n",
    "    3：左边没有事件，当前事件就是第一个事件，直接和1号事件进行合并（但测试集中0号事件长度都长于5帧）\n",
    "    todo: 有可能0号事件和1号事件加起来也不足5帧\n",
    "    Args:\n",
    "        events (_type_): 事件索引，从0开始顺序递增，没有间隔\n",
    "\n",
    "    Returns:\n",
    "        _type_: 融合后的事件索引，每个事件长度大于5帧，但不保证编号连续\n",
    "    \"\"\"\n",
    "    #统计事件中出现次数低于5的事件个数\n",
    "    #统计每个元素的出现次数\n",
    "    counts = Counter(events)\n",
    "    require_count=5\n",
    "    # 找出出现次数不超过5次的元素\n",
    "    elements_with_few_occurrences = [elem for elem, count in counts.items() if count <require_count ]\n",
    "    for element in elements_with_few_occurrences:  #是从所有小于5帧的元素中遍历\n",
    "        # print(element)\n",
    "        if counts[element]>=require_count:  #如果当前事件的长度已经大于5帧，就不需要再合并了,因为后面的操作中会改变（0号事件就不足的情况）\n",
    "            continue\n",
    "        events_start=np.where(events==element)[0][0]\n",
    "        events_end=np.where(events==element)[0][-1]\n",
    "        left_event=element+1\n",
    "        for elem in range(element-1,-1,-1):\n",
    "            if counts[elem]>=require_count: #也有可能一开始第0个事件就不满足5帧\n",
    "                left_event=elem\n",
    "                break\n",
    "        if left_event==element+1: # 如果左边没有找到满足条件的事件\n",
    "            left_event=element-1 #解决一开始第0个事件就不满足5帧的情况\n",
    "            \n",
    "        if element==0:  #第0个事件就长度不足5，但是还是可能第0个和第一个事件之和也不足5帧\n",
    "            left_event=element+1 #直接替换为1号事件,因为得到的事件本身是连续的\n",
    "        \n",
    "        select_event=left_event  \n",
    "        # 直接将events里面的元素值进行修改，后面计算IOU的时候就直接用即可\n",
    "        events[events_start:events_end+1]=len(events[events_start:events_end+1])*[select_event]\n",
    "        counts[select_event]+=counts[element]\n",
    "        del counts[element] #删除当前元素，del 之后，该元素的值就变为0了，因此也不会在向左找的时候找到它\n",
    "    return events  #一开始将return放在了for循环里面，导致只返回了最后一个元素的修改结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 相似度曲线绘制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "video_frames = np.random.rand(1200, 512)\n",
    "\n",
    "def similarity_curve(video_feat,movie_id):\n",
    "    \"\"\"_summary_:绘制相邻帧之间的相似度曲线，以便于观察相似度的变化情况\n",
    "\n",
    "    Args:\n",
    "        video_feat (_type_): 视频帧序的特征\n",
    "        movie_id (_type_): ：选择的是哪个视频，用于创建该视频对应的相似度曲线图保存目录\n",
    "\n",
    "    Returns:\n",
    "        _type_: 每num_frames_per_plot帧的相似度曲线图（视频帧数太多，进行子图绘制）\n",
    "    \"\"\"\n",
    "    # 计算两帧特征之间的相似度（余弦相似度）\n",
    "    def calculate_similarity(feature1, feature2):\n",
    "        dot_product = np.dot(feature1, feature2)\n",
    "        norm_a = np.linalg.norm(feature1)  #L2范数，表示向量的长度\n",
    "        norm_b = np.linalg.norm(feature2)\n",
    "        similarity = dot_product / (norm_a * norm_b) #向量积除以向量长度的乘积等于相似度\n",
    "        return similarity\n",
    "\n",
    "    # 计算相似度,其数量和总帧数差1 \n",
    "    similarities = []\n",
    "    for i in range(len(video_feat) - 1):\n",
    "        similarity = calculate_similarity(video_feat[i], video_feat[i + 1])\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    \n",
    "    # 绘制相似度曲线\n",
    "    # 设置子图参数\n",
    "    num_frames_per_plot = 100  #每一个子图包含多少帧的相似度曲线，为了显示更清楚\n",
    "    num_subplots = len(similarities) // num_frames_per_plot + (1 if len(similarities) % num_frames_per_plot != 0 else 0)\n",
    "\n",
    "    fig, axs = plt.subplots(num_subplots, 1, figsize=(10, num_subplots * 3))\n",
    "\n",
    "    #判断相似度曲线图保存目录是否存在，若不存在则先创建目录\n",
    "    directory_path=os.path.join(\"similarity_curve\",movie_id)\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "        print(f\"Directory '{directory_path}' created.\")\n",
    "    else:\n",
    "        print(f\"Directory '{directory_path}' already exists.\")\n",
    "    # 绘制子图\n",
    "    # 绘制子图并保存\n",
    "    for i in range(num_subplots):\n",
    "        fig, ax = plt.subplots(figsize=(10, 3))\n",
    "        start_index = i * num_frames_per_plot\n",
    "        end_index = min(start_index + num_frames_per_plot, len(similarities))\n",
    "        ax.plot(range(start_index, end_index), similarities[start_index:end_index])\n",
    "        ax.set_xlabel('Frame Index')\n",
    "        ax.set_ylabel('Similarity')\n",
    "        ax.set_xticks(range(start_index, end_index, 2))\n",
    "        ax.axhline(y=0.90, color='r', linestyle='--')\n",
    "        for tick in ax.get_xticklabels():\n",
    "            tick.set_rotation(45)  # 旋转x轴标签\n",
    "        ax.set_title(f'Frame Similarity Curve {start_index + 1} to {end_index}')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # 保存子图\n",
    "        plt.savefig(os.path.join(directory_path, f'frame_similarity_curve_{i + 1}.png'))\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_path='data/mad/annotations/MAD_test.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qids(movie_id,data): #针对一个视频提取所有的query\n",
    "    qids=[]\n",
    "    for key,value in data.items():\n",
    "        # print(value.get(\"movie\"))\n",
    "        if value.get(\"movie\")==movie_id:\n",
    "            # print(key,movie_id)\n",
    "            qids.append(key)\n",
    "    return qids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 计算聚类结果和标注query区间的IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "min_iou=1\n",
    "max_iou=0\n",
    "def get_event_and_compute_iou(movie_id,anno_data,movie_feat):\n",
    "    \"\"\"整体流程为:以电影为单位,计算该电影对应的所有query的iou,只计算一次电影,节省开销\n",
    "    Args:\n",
    "        q_id (string): 为查询的编号\n",
    "        events: 为对应的视频的事件标签\n",
    "\n",
    "    Returns:\n",
    "        list: 与事件的iou\n",
    "    \"\"\"\n",
    "    global min_iou, max_iou  # 声明全局变量\n",
    "    frame_rate=5\n",
    "    qids=get_qids(movie_id,anno_data)\n",
    "    ious=[]\n",
    "    if qids: #因为使用的是不同的数据分割标注,不一定包含该视频\n",
    "        #计算一次视频的事件即可\n",
    "        similarity_threshold=0.9\n",
    "        events=region_growing_event_clustering(movie_feat, similarity_threshold)\n",
    "        events=events_modify(events)\n",
    "        \n",
    "        require_count=5\n",
    "        counts=Counter(events)\n",
    "        # 找出出现次数不超过5次的元素\n",
    "        elements_with_few_occurrences = [elem for elem, count in counts.items() if count <require_count ]\n",
    "        if len(elements_with_few_occurrences)>0:\n",
    "            print(\"modify fail\")\n",
    "            print(elements_with_few_occurrences)\n",
    "        \n",
    "        for q_id in qids: #计算该视频的所有query的timestamps的iou\n",
    "            # print(q_id)\n",
    "            start_time,end_time=data[q_id][\"timestamps\"] #得到的是绝对时间戳\n",
    "            movie_id=data[q_id][\"movie\"]\n",
    "            start_index=round(start_time*frame_rate) #得到绝对帧索引\n",
    "            end_index=round(end_time*frame_rate)\n",
    "            #找到了此query对应的帧特征索引之后,要去匹配最接近的事件区间\n",
    "            nearest_event=events[start_index:end_index+1]  #events和特征帧数是一样的\n",
    "            #统计事件区间中出现次数最多的事件,作为该query匹配的事件\n",
    "            counter = Counter(nearest_event)\n",
    "            main_event = counter.most_common(1)[0][0]\n",
    "            \n",
    "            # print(\"main_event,query: \",main_event,anno_data[q_id][\"sentence\"])\n",
    "            #根据主事件来得到交并比\n",
    "            event_start_index=np.where(events==main_event)[0][0]\n",
    "            event_end_index=np.where(events==main_event)[0][-1]\n",
    "            event_start_time=event_start_index/frame_rate\n",
    "            event_end_time=event_end_index/frame_rate\n",
    "            iou=(min(end_time,event_end_time)-max(start_time,event_start_time))/(max(end_time,event_end_time)-min(start_time,event_start_time))\n",
    "            if iou<0:\n",
    "                iou=1e-5  #当作一个比较小的数\n",
    "            ious.append(iou)\n",
    "            if iou>max_iou:\n",
    "                max_iou=iou\n",
    "            if iou<min_iou:\n",
    "                min_iou=iou\n",
    "    return ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modify fail\n",
      "[-1]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "vfeat_path = '/mnt/hdd1/zhulu/mad/CLIP_B32_frames_features_5fps.h5'  #mad数据集1s采样5帧\n",
    "movie_ious=dict()\n",
    "with h5py.File(vfeat_path, 'r') as f:\n",
    "    for key in f.keys():\n",
    "        movie_id=key\n",
    "        movie_feat=f[key][:]\n",
    "        ious=get_event_and_compute_iou(movie_id,data,movie_feat)\n",
    "        if ious:\n",
    "            # movie_ious.append(sum(ious)/len(ious)) \n",
    "            movie_ious[movie_id]=sum(ious)/len(ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5637360146774332, 0.11537377141697054, 0.4475836703010466, 1e-05, 1.0)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #这个iou计算过程可能有问题，不应该会出现负数，\n",
    " #本身计算的时候就是根据真实区间对应的帧索引中去找到事件，肯定是有iou相交的不应该为负数\n",
    "# min_iou,max_iou \n",
    "ious=movie_ious.values()\n",
    "max(ious),min(ious),sum(ious)/len(ious),min_iou,max_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "json_path='data/mad/annotations/MAD_test.json'\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "vfeat_path = '/mnt/hdd1/zhulu/mad/CLIP_B32_frames_features_5fps.h5'  #mad数据集1s采样5帧\n",
    "f = h5py.File(vfeat_path, 'r')\n",
    "movie_id=\"1012_Unbreakable\"\n",
    "movie_feat=f[movie_id][:]\n",
    "ious=get_event_and_compute_iou(movie_id,data,movie_feat)  #modify只执行了一个数就退出了\n",
    "sum(ious)/len(ious),len(ious)\n",
    "# similarity_curve(f[movie_id][:],movie_id)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.28322061788559505, 577)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ious)/len(ious),len(ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similarity_threshold=0.9\n",
    "events=region_growing_event_clustering(movie_feat, similarity_threshold)\n",
    "events=events_modify(events)\n",
    "# first_60_index = np.where(events == 1)\n",
    "# first_60_index[0][-1],first_60_index[0][0]\n",
    "# events[9255:9400],first_60_index[0][0],first_60_index[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 统计事件的区间长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 6, 2, 10, 4, 5, 17, 8, 13, 59, 3, 20, 11, 21, 24, 23, 18, 28, 35, 45, 30, 50, 14, 54, 9, 37, 97, 12, 113, 19, 15, 1, 63, 42, 25, 62, 34, 31, 32, 58, 112, 100, 27, 33, 16, 64, 114, 22, 109, 26, 47, 39, 174, 65, 72, 29, 67, 92, 56, 57]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "similarity_threshold=0.9\n",
    "events=region_growing_event_clustering(movie_feat, similarity_threshold)\n",
    "events=events_modify(events)\n",
    "counts=Counter(events)\n",
    "time_interval=dict()\n",
    "for key,value in counts.items():\n",
    "    time_span=math.ceil(value/5)  #帧数转换为时间s数\n",
    "    if time_span not in time_interval:\n",
    "        time_interval[time_span]=1\n",
    "    else:\n",
    "        time_interval[time_span]+=1\n",
    "\n",
    "\n",
    "# Extract the keys and values from the dictionary\n",
    "keys = list(time_interval.keys())\n",
    "values = list(time_interval.values())\n",
    "print(keys)\n",
    "# Plot the data\n",
    "plt.bar(keys, values)\n",
    "plt.xticks(ticks=range(min(keys), max(keys) + 1, 5))\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Time Interval')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Time Interval Count')\n",
    "# Display the plot\n",
    "plt.savefig(\"time_interval.png\")  #应该在show之前调用savefig，show之后会清空当前图\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,0,-1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require_count=5\n",
    "# 找出出现次数不超过5次的元素\n",
    "elements_with_few_occurrences = [elem for elem, count in counts.items() if count <require_count ]\n",
    "elements_with_few_occurrences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 33,\n",
       "         3: 29,\n",
       "         4: 6,\n",
       "         5: 49,\n",
       "         7: 10,\n",
       "         8: 18,\n",
       "         9: 22,\n",
       "         10: 82,\n",
       "         12: 29,\n",
       "         13: 19,\n",
       "         15: 36,\n",
       "         17: 65,\n",
       "         18: 292,\n",
       "         22: 15,\n",
       "         25: 49,\n",
       "         26: 96,\n",
       "         27: 27,\n",
       "         28: 23,\n",
       "         29: 16,\n",
       "         30: 18,\n",
       "         32: 16,\n",
       "         34: 16,\n",
       "         35: 21,\n",
       "         36: 12,\n",
       "         37: 20,\n",
       "         38: 17,\n",
       "         39: 22,\n",
       "         40: 12,\n",
       "         41: 47,\n",
       "         43: 18,\n",
       "         44: 22,\n",
       "         45: 21,\n",
       "         46: 55,\n",
       "         48: 24,\n",
       "         49: 50,\n",
       "         52: 18,\n",
       "         53: 19,\n",
       "         54: 103,\n",
       "         56: 18,\n",
       "         58: 16,\n",
       "         59: 36,\n",
       "         60: 23,\n",
       "         61: 9,\n",
       "         64: 33,\n",
       "         66: 22,\n",
       "         68: 10,\n",
       "         69: 118,\n",
       "         71: 111,\n",
       "         72: 23,\n",
       "         73: 89,\n",
       "         75: 117,\n",
       "         76: 19,\n",
       "         77: 98,\n",
       "         79: 24,\n",
       "         81: 32,\n",
       "         83: 22,\n",
       "         85: 21,\n",
       "         86: 24,\n",
       "         87: 11,\n",
       "         88: 21,\n",
       "         89: 13,\n",
       "         91: 6,\n",
       "         92: 140,\n",
       "         93: 19,\n",
       "         94: 12,\n",
       "         95: 26,\n",
       "         96: 20,\n",
       "         97: 9,\n",
       "         98: 28,\n",
       "         99: 30,\n",
       "         100: 24,\n",
       "         101: 23,\n",
       "         103: 9,\n",
       "         105: 14,\n",
       "         109: 16,\n",
       "         110: 33,\n",
       "         111: 16,\n",
       "         113: 28,\n",
       "         114: 29,\n",
       "         115: 26,\n",
       "         116: 12,\n",
       "         117: 9,\n",
       "         118: 14,\n",
       "         119: 6,\n",
       "         120: 23,\n",
       "         122: 26,\n",
       "         123: 139,\n",
       "         127: 173,\n",
       "         129: 114,\n",
       "         133: 221,\n",
       "         135: 6,\n",
       "         136: 86,\n",
       "         138: 20,\n",
       "         141: 52,\n",
       "         143: 31,\n",
       "         144: 39,\n",
       "         154: 147,\n",
       "         155: 87,\n",
       "         156: 35,\n",
       "         158: 31,\n",
       "         163: 100,\n",
       "         164: 27,\n",
       "         165: 11,\n",
       "         166: 11,\n",
       "         167: 28,\n",
       "         168: 21,\n",
       "         169: 39,\n",
       "         170: 32,\n",
       "         171: 30,\n",
       "         172: 52,\n",
       "         175: 14,\n",
       "         183: 18,\n",
       "         192: 7,\n",
       "         194: 248,\n",
       "         195: 12,\n",
       "         197: 30,\n",
       "         199: 24,\n",
       "         201: 55,\n",
       "         202: 68,\n",
       "         206: 62,\n",
       "         210: 8,\n",
       "         212: 8,\n",
       "         215: 16,\n",
       "         216: 268,\n",
       "         217: 36,\n",
       "         218: 23,\n",
       "         219: 6,\n",
       "         221: 35,\n",
       "         222: 42,\n",
       "         223: 182,\n",
       "         224: 19,\n",
       "         225: 65,\n",
       "         226: 482,\n",
       "         227: 60,\n",
       "         229: 44,\n",
       "         230: 35,\n",
       "         231: 563,\n",
       "         232: 92,\n",
       "         233: 71,\n",
       "         234: 26,\n",
       "         235: 8,\n",
       "         236: 23,\n",
       "         241: 16,\n",
       "         242: 19,\n",
       "         245: 5,\n",
       "         246: 8,\n",
       "         248: 10,\n",
       "         250: 11,\n",
       "         251: 25,\n",
       "         252: 16,\n",
       "         253: 14,\n",
       "         254: 8,\n",
       "         255: 59,\n",
       "         256: 27,\n",
       "         257: 149,\n",
       "         258: 20,\n",
       "         259: 314,\n",
       "         260: 28,\n",
       "         262: 23,\n",
       "         263: 31,\n",
       "         264: 45,\n",
       "         265: 208,\n",
       "         266: 14,\n",
       "         267: 33,\n",
       "         268: 122,\n",
       "         269: 37,\n",
       "         270: 22,\n",
       "         271: 70,\n",
       "         272: 149,\n",
       "         273: 309,\n",
       "         274: 168,\n",
       "         275: 154,\n",
       "         276: 156,\n",
       "         277: 31,\n",
       "         278: 288,\n",
       "         279: 6,\n",
       "         280: 11,\n",
       "         281: 25,\n",
       "         282: 118,\n",
       "         284: 16,\n",
       "         285: 158,\n",
       "         286: 21,\n",
       "         287: 18,\n",
       "         289: 13,\n",
       "         290: 18,\n",
       "         291: 18,\n",
       "         299: 12,\n",
       "         304: 10,\n",
       "         308: 21,\n",
       "         313: 14,\n",
       "         314: 25,\n",
       "         315: 27,\n",
       "         316: 28,\n",
       "         317: 11,\n",
       "         318: 57,\n",
       "         319: 28,\n",
       "         321: 7,\n",
       "         322: 556,\n",
       "         323: 29,\n",
       "         324: 62,\n",
       "         325: 170,\n",
       "         326: 30,\n",
       "         327: 26,\n",
       "         328: 56,\n",
       "         330: 40,\n",
       "         331: 61,\n",
       "         332: 27,\n",
       "         333: 14,\n",
       "         334: 39,\n",
       "         335: 29,\n",
       "         336: 27,\n",
       "         337: 29,\n",
       "         338: 42,\n",
       "         340: 500,\n",
       "         341: 155,\n",
       "         343: 7,\n",
       "         344: 17,\n",
       "         345: 19,\n",
       "         346: 42,\n",
       "         348: 7,\n",
       "         349: 19,\n",
       "         350: 5,\n",
       "         351: 34,\n",
       "         352: 24,\n",
       "         353: 23,\n",
       "         354: 27,\n",
       "         360: 12,\n",
       "         361: 27,\n",
       "         369: 5,\n",
       "         370: 9,\n",
       "         371: 5,\n",
       "         372: 13,\n",
       "         375: 57,\n",
       "         378: 6,\n",
       "         380: 7,\n",
       "         381: 52,\n",
       "         382: 13,\n",
       "         383: 34,\n",
       "         384: 10,\n",
       "         388: 6,\n",
       "         390: 5,\n",
       "         391: 5,\n",
       "         392: 10,\n",
       "         393: 22,\n",
       "         397: 10,\n",
       "         403: 12,\n",
       "         404: 19,\n",
       "         407: 5,\n",
       "         408: 28,\n",
       "         409: 6,\n",
       "         410: 41,\n",
       "         411: 22,\n",
       "         414: 6,\n",
       "         415: 20,\n",
       "         416: 41,\n",
       "         417: 64,\n",
       "         418: 26,\n",
       "         419: 12,\n",
       "         422: 30,\n",
       "         423: 19,\n",
       "         424: 82,\n",
       "         426: 22,\n",
       "         428: 5,\n",
       "         429: 33,\n",
       "         430: 131,\n",
       "         431: 58,\n",
       "         432: 22,\n",
       "         433: 11,\n",
       "         434: 52,\n",
       "         435: 17,\n",
       "         436: 57,\n",
       "         437: 9,\n",
       "         438: 147,\n",
       "         439: 162,\n",
       "         440: 32,\n",
       "         441: 22,\n",
       "         442: 33,\n",
       "         443: 113,\n",
       "         444: 16,\n",
       "         445: 35,\n",
       "         446: 78,\n",
       "         447: 16,\n",
       "         448: 43,\n",
       "         449: 20,\n",
       "         450: 37,\n",
       "         451: 39,\n",
       "         452: 44,\n",
       "         453: 185,\n",
       "         454: 34,\n",
       "         455: 29,\n",
       "         456: 44,\n",
       "         457: 206,\n",
       "         458: 320,\n",
       "         460: 18,\n",
       "         461: 6,\n",
       "         462: 23,\n",
       "         463: 17,\n",
       "         464: 566,\n",
       "         465: 33,\n",
       "         466: 92,\n",
       "         471: 13,\n",
       "         474: 13,\n",
       "         475: 5,\n",
       "         476: 21,\n",
       "         478: 23,\n",
       "         481: 35,\n",
       "         482: 5,\n",
       "         483: 17,\n",
       "         485: 32,\n",
       "         486: 25,\n",
       "         487: 107,\n",
       "         488: 83,\n",
       "         489: 16,\n",
       "         490: 42,\n",
       "         491: 15,\n",
       "         492: 8,\n",
       "         493: 76,\n",
       "         494: 29,\n",
       "         498: 160,\n",
       "         499: 6,\n",
       "         500: 21,\n",
       "         501: 41,\n",
       "         504: 48,\n",
       "         505: 23,\n",
       "         507: 22,\n",
       "         508: 131,\n",
       "         509: 22,\n",
       "         510: 18,\n",
       "         511: 52,\n",
       "         512: 544,\n",
       "         513: 313,\n",
       "         514: 82,\n",
       "         515: 30,\n",
       "         517: 23,\n",
       "         518: 45,\n",
       "         519: 25,\n",
       "         522: 103,\n",
       "         524: 15,\n",
       "         525: 64,\n",
       "         527: 20,\n",
       "         529: 126,\n",
       "         530: 26,\n",
       "         531: 59,\n",
       "         532: 18,\n",
       "         535: 5,\n",
       "         536: 82,\n",
       "         541: 98,\n",
       "         542: 231,\n",
       "         543: 287,\n",
       "         544: 30,\n",
       "         545: 22,\n",
       "         546: 172,\n",
       "         547: 191,\n",
       "         548: 66,\n",
       "         550: 74,\n",
       "         552: 81,\n",
       "         556: 116,\n",
       "         559: 35,\n",
       "         560: 51,\n",
       "         561: 11,\n",
       "         562: 13,\n",
       "         566: 25,\n",
       "         567: 24,\n",
       "         568: 867,\n",
       "         569: 29,\n",
       "         570: 61,\n",
       "         571: 20,\n",
       "         572: 15,\n",
       "         573: 82,\n",
       "         574: 324,\n",
       "         575: 358,\n",
       "         576: 34,\n",
       "         577: 129,\n",
       "         578: 317,\n",
       "         579: 46,\n",
       "         580: 93,\n",
       "         581: 10,\n",
       "         582: 8,\n",
       "         583: 11,\n",
       "         587: 25,\n",
       "         589: 13,\n",
       "         592: 23,\n",
       "         594: 7,\n",
       "         595: 18,\n",
       "         598: 5,\n",
       "         599: 11,\n",
       "         602: 13,\n",
       "         603: 19,\n",
       "         605: 26,\n",
       "         606: 64,\n",
       "         608: 6,\n",
       "         610: 12,\n",
       "         611: 19,\n",
       "         612: 13,\n",
       "         613: 8,\n",
       "         614: 24,\n",
       "         615: 7,\n",
       "         617: 15,\n",
       "         618: 5,\n",
       "         619: 129,\n",
       "         628: 31,\n",
       "         630: 8,\n",
       "         631: 75,\n",
       "         632: 60,\n",
       "         634: 9,\n",
       "         635: 118,\n",
       "         636: 120,\n",
       "         637: 104,\n",
       "         638: 8,\n",
       "         639: 25,\n",
       "         643: 19,\n",
       "         647: 19,\n",
       "         648: 22,\n",
       "         651: 12,\n",
       "         654: 131,\n",
       "         655: 18,\n",
       "         657: 24,\n",
       "         658: 26,\n",
       "         663: 34,\n",
       "         669: 143,\n",
       "         674: 23,\n",
       "         675: 26,\n",
       "         676: 6,\n",
       "         677: 44,\n",
       "         680: 5,\n",
       "         681: 22,\n",
       "         691: 130,\n",
       "         693: 12,\n",
       "         694: 40,\n",
       "         698: 8,\n",
       "         699: 10,\n",
       "         703: 12,\n",
       "         708: 14,\n",
       "         711: 6,\n",
       "         712: 15,\n",
       "         714: 136,\n",
       "         716: 16,\n",
       "         720: 15,\n",
       "         721: 53,\n",
       "         722: 25,\n",
       "         723: 52,\n",
       "         724: 46,\n",
       "         725: 15,\n",
       "         726: 80,\n",
       "         727: 54,\n",
       "         728: 10,\n",
       "         729: 23,\n",
       "         730: 77,\n",
       "         731: 155,\n",
       "         732: 9,\n",
       "         734: 181,\n",
       "         735: 44,\n",
       "         736: 7,\n",
       "         737: 70,\n",
       "         738: 46,\n",
       "         739: 14,\n",
       "         740: 50,\n",
       "         741: 20,\n",
       "         742: 11,\n",
       "         743: 184,\n",
       "         744: 6,\n",
       "         745: 16,\n",
       "         746: 22,\n",
       "         748: 15,\n",
       "         749: 18,\n",
       "         750: 34,\n",
       "         752: 21,\n",
       "         755: 18,\n",
       "         757: 24,\n",
       "         759: 32,\n",
       "         760: 19,\n",
       "         761: 14,\n",
       "         762: 40,\n",
       "         763: 12,\n",
       "         764: 17,\n",
       "         768: 7,\n",
       "         770: 16,\n",
       "         775: 14,\n",
       "         777: 16,\n",
       "         779: 32,\n",
       "         780: 22,\n",
       "         781: 20,\n",
       "         782: 32,\n",
       "         783: 13,\n",
       "         784: 11,\n",
       "         785: 11,\n",
       "         786: 13,\n",
       "         787: 6,\n",
       "         788: 26,\n",
       "         789: 8,\n",
       "         790: 6,\n",
       "         791: 13,\n",
       "         792: 11,\n",
       "         794: 6,\n",
       "         795: 13,\n",
       "         798: 8,\n",
       "         800: 6,\n",
       "         802: 13,\n",
       "         806: 10,\n",
       "         809: 11,\n",
       "         810: 20,\n",
       "         811: 5,\n",
       "         812: 8,\n",
       "         813: 13,\n",
       "         816: 12,\n",
       "         818: 8,\n",
       "         819: 37,\n",
       "         820: 25,\n",
       "         821: 5,\n",
       "         822: 169,\n",
       "         823: 26,\n",
       "         824: 68,\n",
       "         827: 22,\n",
       "         828: 25,\n",
       "         829: 43,\n",
       "         830: 18,\n",
       "         831: 10,\n",
       "         833: 19,\n",
       "         834: 9,\n",
       "         836: 17,\n",
       "         840: 28,\n",
       "         843: 8,\n",
       "         844: 334,\n",
       "         845: 11,\n",
       "         846: 459,\n",
       "         847: 167,\n",
       "         848: 94,\n",
       "         849: 63,\n",
       "         850: 98,\n",
       "         851: 60,\n",
       "         852: 8,\n",
       "         853: 31,\n",
       "         854: 33,\n",
       "         855: 37,\n",
       "         856: 31,\n",
       "         857: 32,\n",
       "         858: 34,\n",
       "         859: 30,\n",
       "         860: 29,\n",
       "         861: 30,\n",
       "         863: 34,\n",
       "         868: 32,\n",
       "         869: 280,\n",
       "         870: 247,\n",
       "         871: 312,\n",
       "         873: 89,\n",
       "         875: 101,\n",
       "         877: 59,\n",
       "         880: 22,\n",
       "         881: 36,\n",
       "         882: 40,\n",
       "         883: 8,\n",
       "         885: 14,\n",
       "         886: 9,\n",
       "         887: 22,\n",
       "         888: 51,\n",
       "         889: 17,\n",
       "         890: 74,\n",
       "         891: 19,\n",
       "         892: 21,\n",
       "         893: 29,\n",
       "         894: 47,\n",
       "         895: 29,\n",
       "         896: 71,\n",
       "         897: 17,\n",
       "         898: 12,\n",
       "         899: 18,\n",
       "         900: 19,\n",
       "         901: 77,\n",
       "         902: 31,\n",
       "         903: 12,\n",
       "         904: 38,\n",
       "         905: 32,\n",
       "         906: 19,\n",
       "         907: 22,\n",
       "         909: 19,\n",
       "         910: 79,\n",
       "         925: 42,\n",
       "         926: 130,\n",
       "         927: 22,\n",
       "         928: 281,\n",
       "         932: 12,\n",
       "         934: 81,\n",
       "         935: 6,\n",
       "         936: 51,\n",
       "         940: 53,\n",
       "         947: 11,\n",
       "         949: 70,\n",
       "         951: 31,\n",
       "         958: 16,\n",
       "         960: 12,\n",
       "         961: 18,\n",
       "         964: 7,\n",
       "         966: 6,\n",
       "         968: 12,\n",
       "         969: 49,\n",
       "         993: 115,\n",
       "         1061: 17,\n",
       "         1065: 19,\n",
       "         1066: 12,\n",
       "         1069: 12,\n",
       "         1071: 12,\n",
       "         1072: 18,\n",
       "         1073: 19,\n",
       "         1074: 12,\n",
       "         1077: 29,\n",
       "         1078: 12})"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "events\n",
    "counts = Counter(events)\n",
    "counts\n",
    "# Convert counts to a dictionary\n",
    "# counts_dict = dict(counts)\n",
    "# counts_dict = {str(k): v for k, v in counts.items()}\n",
    "# # Write counts to a JSON file\n",
    "# with open('counts.json', 'w') as f:\n",
    "#     json.dump(counts_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events\n",
    "counts = Counter(events)\n",
    "# 找出出现次数不超过5次的元素\n",
    "elements_with_few_occurrences = [elem for elem, count in counts.items() if count <= 5]\n",
    "elements_with_few_occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3, 3, 3, 3, 4, 5, 6, 7], 4)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=[0,1,2,3,4,5,6,7]\n",
    "x[0:4]=[3]*len(x[0:4])\n",
    "x,len(x[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82389915"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vfeat_path = '/mnt/hdd1/zhulu/mad/CLIP_B32_frames_features_5fps.h5'  #mad数据集1s采样5帧\n",
    "f = h5py.File(vfeat_path, 'r')\n",
    "movie_id=\"1012_Unbreakable\"\n",
    "Mfeats=f[movie_id][:]\n",
    "feature1=Mfeats[3830]\n",
    "feature2=Mfeats[3831]\n",
    "dot_product = np.dot(feature1, feature2)\n",
    "norm_a = np.linalg.norm(feature1)  #L2范数，表示向量的长度\n",
    "norm_b = np.linalg.norm(feature2)\n",
    "similarity = dot_product / (norm_a * norm_b) #向量积除以向量长度的乘积等于相似度\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Write the dictionary to a file\n",
    "with open('movie_ious.json', 'w') as f:\n",
    "    json.dump(movie_ious, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event class loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### `输入`：1：一维向量，值单调不减，表示属于第几个事件，索引表示是视频中的第几帧；2：N表示需要对齐到多少帧（多于N帧，进行下采样，少于N帧不处理）\n",
    "###### `输出`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned events: tensor([0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4])\n",
      "Aligned indices: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10,  9, 14, 13, 11, 16, 21, 17, 20,\n",
      "        19])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def resample_event_indices(indices, N):\n",
    "    num_frames = len(indices)\n",
    "    #短事件进行补齐\n",
    "    # if num_frames < N:\n",
    "    #     # 循环重复补齐帧数\n",
    "    #     repeat_times = N // num_frames\n",
    "    #     remainder = N % num_frames\n",
    "    #     resampled_indices = indices.repeat(repeat_times)\n",
    "    #     if remainder > 0:\n",
    "    #         resampled_indices = torch.cat((resampled_indices, indices[:remainder]))\n",
    "    if num_frames > N:\n",
    "        # 随机下采样\n",
    "        resampled_indices = indices[torch.randperm(num_frames)[:N]]\n",
    "    else:\n",
    "        resampled_indices = indices\n",
    "    return resampled_indices\n",
    "\n",
    "def align_event_frames(events, N):\n",
    "    unique_events = torch.unique(events)\n",
    "    event_to_indices = {event.item(): (events == event).nonzero(as_tuple=True)[0] for event in unique_events}\n",
    "\n",
    "    # 对每个事件的帧索引进行对齐，并保留对齐后的索引信息\n",
    "    aligned_indices = []\n",
    "    for event, indices in event_to_indices.items():\n",
    "        resampled_indices = resample_event_indices(indices, N)\n",
    "        aligned_indices.append(resampled_indices)\n",
    "\n",
    "    # 将对齐后的索引展平，并按帧索引排序\n",
    "    aligned_indices = torch.cat(aligned_indices)\n",
    "\n",
    "    # 创建一个新的事件数组，大小与对齐后的索引一致\n",
    "    aligned_events = torch.zeros(len(aligned_indices), dtype=events.dtype)\n",
    "    for new_index, old_index in enumerate(aligned_indices):\n",
    "        aligned_events[new_index] = events[old_index]\n",
    "\n",
    "    return aligned_events, aligned_indices\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == \"__main__\":\n",
    "    # 示例输入\n",
    "    events = torch.tensor([0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4])  # 事件编号数组\n",
    "    N = 5  # 目标帧数\n",
    "\n",
    "    aligned_events, aligned_indices = align_event_frames(events, N)\n",
    "    print(\"Aligned events:\", aligned_events)\n",
    "    print(\"Aligned indices:\", aligned_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### `损失`:事件内帧相似度高，事件间帧相似度低"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simi:  torch.Size([5])\n",
      "Loss: 9.615434646606445\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_event_representatives(feats, events):\n",
    "    unique_events = torch.unique(events)  \n",
    "    start_feats = []\n",
    "    mean_feats = []\n",
    "    end_feats = []\n",
    "\n",
    "    for event in unique_events:\n",
    "        #返回各个事件的所包含的索引位置\n",
    "        event_indices = (events == event).nonzero(as_tuple=True)[0]\n",
    "        if len(event_indices) > 0:\n",
    "            # print(\"shape: \",feats[event_indices[0]].unsqueeze(0).shape)\n",
    "            start_feats.append(feats[event_indices[0]].unsqueeze(0)) #（1，512）\n",
    "            end_feats.append(feats[event_indices[-1]].unsqueeze(0))\n",
    "            mean_feats.append(feats[event_indices].mean(dim=0).unsqueeze(0))\n",
    "    \n",
    "    return torch.cat(start_feats), torch.cat(mean_feats), torch.cat(end_feats)\n",
    "\n",
    "def calculate_similarity_loss(feats, events):\n",
    "    #start_feats表示每个事件的第一帧特征，mean_feats表示每个事件的平均特征，\n",
    "    #end_feats表示每个事件的最后一帧特征，向量长度表示事件个数\n",
    "    start_feats, mean_feats, end_feats = get_event_representatives(feats, events)\n",
    "    # print(\"start_feats: \", start_feats.shape)\n",
    "    \n",
    "    # Same event similarity (maximize)\n",
    "    same_event_loss = 0.0\n",
    "    print(\"simi: \",F.cosine_similarity(start_feats, mean_feats).shape)\n",
    "    #是两个张量之间每行元素进行对应，每一行表示一个事件，因此计算等价于\n",
    "    #第一个事件的开始特征和第一个事件的平均特征的余弦相似度\n",
    "    same_event_loss += torch.sum(1 - F.cosine_similarity(start_feats, mean_feats))\n",
    "    same_event_loss += torch.sum(1 - F.cosine_similarity(mean_feats, end_feats))\n",
    "    same_event_loss += torch.sum(1 - F.cosine_similarity(start_feats, end_feats))\n",
    "    \n",
    "    # Adjacent event dissimilarity (minimize similarity)\n",
    "    adjacent_event_loss = 0.0\n",
    "\n",
    "    # Calculate cosine similarity for adjacent events\n",
    "    for i in range(start_feats.shape[0] - 1):\n",
    "        #虽然没有显示计算当前事件和前一个事件之间关于三个代表帧之间的相似度，但是通过累加计算步骤\n",
    "        #能够隐含的实现该功能\n",
    "        current_start, current_mean, current_end = start_feats[i], mean_feats[i], end_feats[i]\n",
    "        next_start, next_mean, next_end = start_feats[i + 1], mean_feats[i + 1], end_feats[i + 1]\n",
    "\n",
    "        # Concatenate all current and next features\n",
    "        current_feats = torch.cat((current_start.unsqueeze(0), current_mean.unsqueeze(0), current_end.unsqueeze(0)))\n",
    "        next_feats = torch.cat((next_start.unsqueeze(0), next_mean.unsqueeze(0), next_end.unsqueeze(0)))\n",
    "\n",
    "        # Calculate cosine similarity between all pairs of current and next features\n",
    "        similarity_matrix = F.cosine_similarity(current_feats.unsqueeze(1), next_feats.unsqueeze(0), dim=2)\n",
    "        \n",
    "        adjacent_event_loss -= similarity_matrix.sum()\n",
    "\n",
    "    total_loss = same_event_loss + adjacent_event_loss\n",
    "    return total_loss\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == \"__main__\":\n",
    "    # 示例输入\n",
    "    events = torch.tensor([0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4])\n",
    "    feats = torch.randn(len(events), 512)\n",
    "\n",
    "    loss = calculate_similarity_loss(feats, events)\n",
    "    print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### `抽象事件表达`:将上述得到的长度不一的事件单元进行抽象表示，以统一每个事件的维度。\n",
    "###### （如：有的短事件仅包含5帧，但有的长事件包含100帧，为了便于后续的批量计算，将每个事件的维度进行对齐，全部变为【N,M】，其中N表示抽象的概念，不同的事件虽然帧数不同，但是都需要人物、物品、动作、环境等）\n",
    "###### 用attention计算，设置可学习的query数量为num_queries，这些query与事件交互，得到抽象语义信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract features shape: torch.Size([15, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionModule(torch.nn.Module):\n",
    "    def __init__(self, num_queries, input_dim):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.num_queries = num_queries\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # 定义查询向量\n",
    "        self.queries = torch.nn.Parameter(torch.randn(num_queries, input_dim))\n",
    "        \n",
    "        # 线性层，将帧特征映射到注意力分数\n",
    "        self.linear = torch.nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def forward(self, event_unit):\n",
    "        # event_unit 的维度应为 (num_frames, input_dim)\n",
    "        num_frames = event_unit.size(0)\n",
    "        \n",
    "        # 计算查询向量与帧特征的注意力分数\n",
    "        # queries(num_queries,hidden dim) event_unit(num_frames,hidden dim) -> (num_queries,num_frames)\n",
    "        query_scores = torch.matmul(self.queries, event_unit.T) \n",
    "        \n",
    "        # 计算注意力权重\n",
    "        # 每一行上进行 softmax，得到每个查询对每帧的注意力权重(合理)\n",
    "        weights = F.softmax(query_scores, dim=1) # 在每个查询向量的维度上进行 softmax\n",
    "        \n",
    "        # 加权平均每帧的特征，得到每个查询的抽象特征\n",
    "        #weights(num_queries,num_frames) event_unit(num_frames,hidden dim) -> (num_queries,hidden\n",
    "        attended_features = torch.matmul(weights, event_unit)  # (num_queries, input_dim)\n",
    "        \n",
    "        # 返回结果\n",
    "        return attended_features\n",
    "\n",
    "# 示例用法\n",
    "input_dim = 512\n",
    "num_queries = 15  # 事件中抽象概念的学习，比如人物、动作、场景、动作等\n",
    "\n",
    "# 创建注意力模块实例\n",
    "attention_module = AttentionModule(num_queries, input_dim)\n",
    "\n",
    "# 生成示例数据，假设事件单元有100帧，每帧特征为512维\n",
    "event_unit = torch.randn(119, input_dim)\n",
    "# 虽然可以处理不同帧长的事件序列，都编码成15，512，但是不能一次性批量计算，在pytorch中tensor张量要保持矩阵格式\n",
    "# 因此可能还是需要按长度分批处理，这种方式会打乱顺序，可能需要对原本的索引进行一定的保存\n",
    "\n",
    "# 使用注意力模块进行前向传播\n",
    "abstract_features = attention_module(event_unit)\n",
    "\n",
    "# 打印结果形状，预期输出形状为 (num_queries, input_dim)\n",
    "print(\"Abstract features shape:\", abstract_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 采样器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import math\n",
    "from typing import TypeVar, Optional, Iterator\n",
    "from torch.utils.data import Sampler, Dataset\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "T_co = TypeVar('T_co', covariant=True)\n",
    "\n",
    "class MyDistributedSampler(Sampler[T_co]):\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        dataset: Dataset used for sampling.\n",
    "        num_replicas (int, optional): Number of processes participating in\n",
    "            distributed training. By default, :attr:`world_size` is retrieved from the\n",
    "            current distributed group. 表示num_replicas等价于world_size\n",
    "        rank (int, optional): Rank of the current process within :attr:`num_replicas`.\n",
    "            By default, :attr:`rank` is retrieved from the current distributed\n",
    "            group.\n",
    "        shuffle (bool, optional): If ``True`` (default), sampler will shuffle the\n",
    "            indices.\n",
    "        seed (int, optional): random seed used to shuffle the sampler if\n",
    "            :attr:`shuffle=True`. This number should be identical across all\n",
    "            processes in the distributed group. Default: ``0``.\n",
    "        drop_last (bool, optional): if ``True``, then the sampler will drop the\n",
    "            tail of the data to make it evenly divisible across the number of\n",
    "            replicas. If ``False``, the sampler will add extra indices to make\n",
    "            the data evenly divisible across the replicas. Default: ``False``.\n",
    "\n",
    "    .. warning::\n",
    "        In distributed mode, calling the :meth:`set_epoch` method at\n",
    "        the beginning of each epoch **before** creating the :class:`DataLoader` iterator\n",
    "        is necessary to make shuffling work properly across multiple epochs. Otherwise,\n",
    "        the same ordering will be always used.\n",
    "\n",
    "    Example::\n",
    "\n",
    "        >>> # xdoctest: +SKIP\n",
    "        >>> sampler = DistributedSampler(dataset) if is_distributed else None\n",
    "        >>> loader = DataLoader(dataset, shuffle=(sampler is None),\n",
    "        ...                     sampler=sampler)\n",
    "        >>> for epoch in range(start_epoch, n_epochs):\n",
    "        ...     if is_distributed:\n",
    "        ...         sampler.set_epoch(epoch)\n",
    "        ...     train(loader)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: Dataset, num_replicas: Optional[int] = None,\n",
    "                 rank: Optional[int] = None, shuffle: bool = True,\n",
    "                 seed: int = 0, drop_last: bool = False) -> None:\n",
    "        if num_replicas is None:\n",
    "            if not dist.is_available():\n",
    "                raise RuntimeError(\"Requires distributed package to be available\")\n",
    "            num_replicas = dist.get_world_size()\n",
    "        if rank is None:\n",
    "            if not dist.is_available():\n",
    "                raise RuntimeError(\"Requires distributed package to be available\")\n",
    "            rank = dist.get_rank()\n",
    "        if rank >= num_replicas or rank < 0:\n",
    "            raise ValueError(\n",
    "                \"Invalid rank {}, rank should be in the interval\"\n",
    "                \" [0, {}]\".format(rank, num_replicas - 1))\n",
    "        self.dataset = dataset\n",
    "        self.num_replicas = num_replicas\n",
    "        self.rank = rank\n",
    "        self.epoch = 0\n",
    "        self.drop_last = drop_last\n",
    "        # If the dataset length is evenly divisible by # of replicas, then there\n",
    "        # is no need to drop any data, since the dataset will be split equally.\n",
    "        if self.drop_last and len(self.dataset) % self.num_replicas != 0:  # type: ignore[arg-type]\n",
    "            \n",
    "            self.num_samples = math.ceil(\n",
    "                (len(self.dataset) - self.num_replicas) / self.num_replicas  # type: ignore[arg-type]\n",
    "            )\n",
    "        else: #不需要丢弃数据，使用全部数据\n",
    "            self.num_samples = math.ceil(len(self.dataset) / self.num_replicas)  \n",
    "        self.total_size = self.num_samples * self.num_replicas #在不丢弃数据的情况下，total_size在不整除的情况下会变大\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "\n",
    "    def __iter__(self) -> Iterator[T_co]:\n",
    "        if self.shuffle:\n",
    "            # deterministically shuffle based on epoch and seed\n",
    "            g = torch.Generator() #创造一个随机数生成器\n",
    "            g.manual_seed(self.seed + self.epoch)\n",
    "            indices = torch.randperm(len(self.dataset), generator=g).tolist()  # type: ignore[arg-type]\n",
    "        else:\n",
    "            indices = list(range(len(self.dataset)))  # type: ignore[arg-type]\n",
    "\n",
    "        if not self.drop_last:  #表示要使用所有的数据\n",
    "            # add extra samples to make it evenly divisible\n",
    "            padding_size = self.total_size - len(indices)\n",
    "            if padding_size <= len(indices):\n",
    "                indices += indices[:padding_size]\n",
    "            else:\n",
    "                indices += (indices * math.ceil(padding_size / len(indices)))[:padding_size]\n",
    "        else:\n",
    "            # remove tail of data to make it evenly divisible.\n",
    "            indices = indices[:self.total_size]\n",
    "        assert len(indices) == self.total_size\n",
    "        #上述实现对数据丢弃与否的处理\n",
    "\n",
    "        # subsample\n",
    "        indices = indices[self.rank:self.total_size:self.num_replicas]\n",
    "        assert len(indices) == self.num_samples\n",
    "\n",
    "        return iter(indices)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n",
    "    \n",
    "\n",
    "\n",
    "    def set_epoch(self, epoch: int) -> None:\n",
    "        r\"\"\"\n",
    "        Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas\n",
    "        use a different random ordering for each epoch. Otherwise, the next iteration of this\n",
    "        sampler will yield the same ordering.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): Epoch number.\n",
    "        \"\"\"\n",
    "        self.epoch = epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "# 自定义数据集示例\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "data = list(range(100))\n",
    "dataset = CustomDataset(data)\n",
    "\n",
    "# 初始化 DistributedSampler\n",
    "sampler = MyDistributedSampler(dataset)\n",
    "\n",
    "# 创建 DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
    "\n",
    "# 使用 DataLoader 进行训练或评估\n",
    "for batch in dataloader:\n",
    "    # 在这里添加训练或评估逻辑\n",
    "    print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.ceil(10 / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "# 自定义数据集示例\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "# 自定义 DistributedSampler\n",
    "class MyDistributedSampler(DistributedSampler):\n",
    "    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True):\n",
    "        super().__init__(dataset, num_replicas=num_replicas, rank=rank, shuffle=shuffle)\n",
    "        # 在这里可以添加自定义的初始化逻辑\n",
    "\n",
    "    def __iter__(self):\n",
    "        # 这里可以自定义采样器的迭代逻辑，例如修改采样的顺序等\n",
    "        indices = list(super().__iter__())\n",
    "        # 可以在这里加入自己的采样逻辑，比如重新排序\n",
    "        return iter(indices)\n",
    "\n",
    "# 示例用法\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建自定义数据集\n",
    "    data = list(range(100))\n",
    "    dataset = CustomDataset(data)\n",
    "\n",
    "    # 初始化 DistributedSampler\n",
    "    sampler = MyDistributedSampler(dataset)\n",
    "\n",
    "    # 创建 DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
    "\n",
    "    # 使用 DataLoader 进行训练或评估\n",
    "    for batch in dataloader:\n",
    "        # 在这里添加训练或评估逻辑\n",
    "        print(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PSVL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
